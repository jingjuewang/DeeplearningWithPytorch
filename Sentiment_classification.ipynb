{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- use a better tokenization\n",
    "\n",
    "-- you may need to change some of the parameters (for example sentence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!"
     ]
    }
   ],
   "source": [
    "! head aclImdb/train/pos/0_9.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos = os.listdir(\"aclImdb/train/pos/\")\n",
    "train_neg = os.listdir(\"aclImdb/train/neg/\")\n",
    "test_pos = os.listdir(\"aclImdb/test/pos/\")\n",
    "test_neg = os.listdir(\"aclImdb/test/neg/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_contents(dir_path, file_list):\n",
    "    content_list = []\n",
    "    for f in file_list:\n",
    "        with open(os.path.join(dir_path, f), \"r\") as file:\n",
    "            content = file.read()\n",
    "            content_list.append(content)\n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_content = read_contents(\"aclImdb/train/pos/\", train_pos)\n",
    "train_neg_content = read_contents(\"aclImdb/train/neg/\", train_neg)\n",
    "test_pos_content = read_contents(\"aclImdb/test/pos/\", test_pos)\n",
    "test_neg_content = read_contents(\"aclImdb/test/neg/\", test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_content = train_pos_content + train_neg_content\n",
    "test_content = test_pos_content + test_neg_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos = pd.DataFrame({\"content\": train_pos_content,\n",
    "                         \"pos_neg\": 1})\n",
    "train_neg = pd.DataFrame({\"content\": train_neg_content,\n",
    "                         \"pos_neg\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_pos.append(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pos = pd.DataFrame({\"content\": test_pos_content,\n",
    "                         \"pos_neg\": 1})\n",
    "test_neg = pd.DataFrame({\"content\": test_neg_content,\n",
    "                         \"pos_neg\": 0})\n",
    "test = test_pos.append(test_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "def sub_br(x): \n",
    "    return re_br.sub(\"\\n\", x)\n",
    "\n",
    "def remove_punctuations(x):\n",
    "    for c in string.punctuation:\n",
    "        x = x.replace(c,\"\")\n",
    "    return x\n",
    "\n",
    "my_tok = spacy.load(\"en\")\n",
    "def spacy_tok(x): \n",
    "    return [tok.text for tok in my_tok.tokenizer(sub_br(remove_punctuations(x)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_non_stopwords(content):\n",
    "    \"\"\"Returns a list of non-stopwords\"\"\"\n",
    "    return {x:1 for x in spacy_tok(str(content).lower()) if x not in stops}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(list_of_content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = get_non_stopwords(list_of_content)\n",
    "    words = [x for x in spacy_tok(str(list_of_content).lower())]\n",
    "    for w in words:\n",
    "        if w in vocab:\n",
    "            vocab[w] += 1\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_vocab = get_vocab(train_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile=\"glove.6B.300d.txt\"):\n",
    "    \"\"\" Loads word vectors into a dictionary.\"\"\"\n",
    "    f = open(gloveFile,'r')\n",
    "    word_vecs = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        word_vecs[word] = np.array([float(val) for val in splitLine[1:]])\n",
    "    return word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = loadGloveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_rare_words(embeddings, data_vocab, min_df=2):\n",
    "    words_delete = []\n",
    "    for word in data_vocab:\n",
    "        if data_vocab[word] < min_df and word not in embeddings:\n",
    "            words_delete.append(word)\n",
    "#     print(words_delete)\n",
    "    for word in words_delete: data_vocab.pop(word)\n",
    "    return data_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_vocab = delete_rare_words(embeddings, data_vocab, min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(embeddings, data_vocab, min_df=2, D=300):\n",
    "    \"\"\"Creates embedding matrix from word vectors. \"\"\"\n",
    "    data_vocab = delete_rare_words(embeddings, data_vocab, min_df)\n",
    "    V = len(data_vocab.keys()) + 2\n",
    "    vocab2index = {}\n",
    "    W = np.zeros((V, D), dtype=\"float32\")\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    # adding a vector for padding\n",
    "    W[0] = np.zeros(D, dtype='float32')\n",
    "    # adding a vector for rare words \n",
    "    W[1] = np.random.uniform(-0.25,0.25,D)\n",
    "    vocab2index[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in data_vocab:\n",
    "        if word in embeddings:\n",
    "            W[i] = embeddings[word]\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25,D)\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1   \n",
    "    return W, np.array(vocab), vocab2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_weight, vocab, vocab2index = create_embedding_matrix(embeddings, data_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121499"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrained_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0556,  0.2216, -0.2337,  ..., -0.0201,  0.1580, -0.1758],\n",
       "        [ 0.1325, -0.3820,  0.0549,  ..., -0.2868,  0.2870,  0.3807],\n",
       "        ...,\n",
       "        [ 0.4018, -0.2759, -0.1173,  ..., -0.2185,  0.0948, -0.1024],\n",
       "        [-0.0162, -0.5028,  0.2609,  ...,  0.1186,  0.2515, -0.0861],\n",
       "        [-0.0203, -0.0016,  0.1310,  ...,  0.1015,  0.0639,  0.0967]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = 300\n",
    "V = len(pretrained_weight)\n",
    "emb = nn.Embedding(V, D)\n",
    "emb.weight.data.copy_(torch.from_numpy(pretrained_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_sentence(s, N=40):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s.split()])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 40)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.stack([encode_sentence(x) for x in train.content.values])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 40)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.stack([encode_sentence(x) for x in test.content.values])\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentenceCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, V, D, glove_weights):\n",
    "        super(SentenceCNN, self).__init__()\n",
    "        self.glove_weights = glove_weights\n",
    "        self.embedding = nn.Embedding(V, D, padding_idx=0)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(self.glove_weights))\n",
    "        self.embedding.weight.requires_grad = False ## freeze embeddings\n",
    "\n",
    "        self.conv_3 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=3)\n",
    "        self.conv_4 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=4)\n",
    "        self.conv_5 = nn.Conv1d(in_channels=D, out_channels=100, kernel_size=5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(300, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x3 = F.relu(self.conv_3(x))\n",
    "        x4 = F.relu(self.conv_4(x))\n",
    "        x5 = F.relu(self.conv_5(x))\n",
    "        x3 = nn.MaxPool1d(kernel_size = 38)(x3)\n",
    "        x4 = nn.MaxPool1d(kernel_size = 37)(x4)\n",
    "        x5 = nn.MaxPool1d(kernel_size = 36)(x5)\n",
    "        out = torch.cat([x3, x4, x5], 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(pretrained_weight)\n",
    "D = 300\n",
    "N = 100\n",
    "model = SentenceCNN(V, D, glove_weights=pretrained_weight).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train.pos_neg.values.astype(np.float32)\n",
    "y_test = test.pos_neg.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WiDSDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x = x.copy()\n",
    "        self.x = x.copy().astype(np.int64)\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return [self.x[idx], self.y[idx]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ds = WiDSDataset(x_train, y_train)\n",
    "test_ds = WiDSDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr=0.01, wd=0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_dl=train_dl, verbose=False):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for i, (x, y) in enumerate(train_dl):\n",
    "        batch = y.shape[0]\n",
    "        x = torch.LongTensor(x).cuda()\n",
    "        y = torch.Tensor(y).unsqueeze(1).cuda()\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += batch\n",
    "        sum_loss += batch * (loss.item())\n",
    "        if verbose: \n",
    "            print(sum_loss/total)\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_loss(model, test_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    accuracy = 0\n",
    "    for i, (x, y) in enumerate(test_dl):\n",
    "        batch = y.shape[0]\n",
    "        x = torch.LongTensor(x).cuda()\n",
    "        y = torch.Tensor(y).unsqueeze(1).cuda()\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        sum_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        pred = (y_hat > 0).float()\n",
    "        correct += (pred == y).float().sum().item()\n",
    "        accuracy += correct/pred.shape[0]\n",
    "    print(\"test loss and accuracy\", sum_loss/total, accuracy/total)\n",
    "    return sum_loss/total, accuracy/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_loop(model, epochs, lr=0.01, wd=0.0):\n",
    "    optim = get_optimizer(model, lr=lr, wd=wd)\n",
    "    for i in range(epochs):\n",
    "        loss = train_model(model, optim, train_dl)\n",
    "        print(\"loss\", loss)\n",
    "        test_loss(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 8.831651431482427\n",
      "test loss and accuracy 0.6948733234405517 0.00338464\n",
      "loss 0.702161238193512\n",
      "test loss and accuracy 0.6957425785064697 0.00338324\n",
      "loss 0.7027262687683106\n",
      "test loss and accuracy 0.6952866411209107 0.003398\n",
      "loss 0.6986085176467896\n",
      "test loss and accuracy 0.6947899055480957 0.0034235199999999998\n",
      "loss 0.6969630908966065\n",
      "test loss and accuracy 0.6944407200813294 0.00346036\n",
      "loss 0.6949128079414367\n",
      "test loss and accuracy 0.6937753248214722 0.00348728\n",
      "loss 0.6934193348884583\n",
      "test loss and accuracy 0.6928571820259094 0.0035606000000000006\n",
      "loss 0.6900584936141968\n",
      "test loss and accuracy 0.6905372643470764 0.0035962800000000003\n",
      "loss 0.6947833442687988\n",
      "test loss and accuracy 0.6894394063949585 0.0036276\n",
      "loss 0.6867070269584655\n",
      "test loss and accuracy 0.6881621956825257 0.00363664\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7933547520637512\n",
      "test loss and accuracy 0.6943894982337951 0.00855564\n",
      "loss 0.6793985319137573\n",
      "test loss and accuracy 0.6870004105567932 0.004048160000000001\n",
      "loss 0.6667534232139587\n",
      "test loss and accuracy 0.6876257109642029 0.00399256\n",
      "loss 0.6726556468009949\n",
      "test loss and accuracy 0.688574333190918 0.0043378\n",
      "loss 0.656980881690979\n",
      "test loss and accuracy 0.691621994972229 0.004226400000000001\n",
      "loss 0.662782793045044\n",
      "test loss and accuracy 0.69141667842865 0.00435188\n",
      "loss 0.6409912300109863\n",
      "test loss and accuracy 0.6912966108322144 0.0042403200000000005\n",
      "loss 0.6514074206352234\n",
      "test loss and accuracy 0.6970428490638733 0.0043565999999999995\n",
      "loss 0.6712064123153687\n",
      "test loss and accuracy 0.7102455520629882 0.00408972\n",
      "loss 0.6411055088043213\n",
      "test loss and accuracy 0.921364893913269 0.003679639999999999\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=10, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.8706991934776306\n",
      "test loss and accuracy 0.7442146515846253 0.00587988\n",
      "loss 0.693601312637329\n",
      "test loss and accuracy 0.7111865544319153 0.007054760000000001\n",
      "loss 0.6328119778633118\n",
      "test loss and accuracy 0.7007159185409546 0.00786776\n",
      "loss 0.6035014247894287\n",
      "test loss and accuracy 0.695798864364624 0.00824464\n",
      "loss 0.5871456170082092\n",
      "test loss and accuracy 0.6939116406440735 0.00850172\n",
      "loss 0.581382486820221\n",
      "test loss and accuracy 0.6928787350654602 0.00868556\n",
      "loss 0.5702439308166504\n",
      "test loss and accuracy 0.6939883470535279 0.00876876\n",
      "loss 0.5624044585227966\n",
      "test loss and accuracy 0.6940492343902588 0.008844520000000002\n",
      "loss 0.5531093716621399\n",
      "test loss and accuracy 0.695306568145752 0.00885936\n",
      "loss 0.5504364442825317\n",
      "test loss and accuracy 0.6957342028617859 0.008918639999999999\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=10, lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
